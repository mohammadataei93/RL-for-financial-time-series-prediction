{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3-tf-agents-trade-env.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Qwyv-Yr_ipz",
        "colab_type": "text"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoOttHgv-G0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install 'gym==0.10.11'\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install PILLOW\n",
        "!pip install 'pyglet==1.3.2'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install --pre tf-agents[reverb]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5lEqfr4_zDF",
        "colab_type": "text"
      },
      "source": [
        "Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZiglrfZ_h3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "import tensorflow as tf\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import q_network, q_rnn_network\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xZo2iY_AGaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.enable_v2_behavior()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlGWjixhANd3",
        "colab_type": "text"
      },
      "source": [
        "StockMarket Environment Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKpZa2N8AMlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import abc\n",
        "from enum import Enum\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import wrappers\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "import pandas as pd\n",
        "tf.compat.v1.enable_v2_behavior()\n",
        "\n",
        "\n",
        "class StockMarket(py_environment.PyEnvironment):\n",
        "\n",
        "  def __init__(self, dataset, start_date, end_date, window_size):\n",
        "      self._prices = dataset[start_date:end_date]['Close'].values\n",
        "      self._window_size = window_size\n",
        "      self._start_index = window_size-1\n",
        "      self._end_index = len(self._prices) - 2\n",
        "      self._current_index = self._start_index\n",
        "\n",
        "\n",
        "      self._action_spec = array_spec.BoundedArraySpec(\n",
        "          shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
        "      self._observation_spec = array_spec.BoundedArraySpec(\n",
        "          shape=(self._window_size,), dtype=np.float32, minimum=-1, maximum=1, name='observation')\n",
        "      self._state = self._get_observation()\n",
        "      self._episode_ended = False\n",
        "\n",
        "  def _get_observation(self):\n",
        "    observ=self._prices[self._current_index-self._window_size+1:self._current_index+1]\n",
        "    normalized_window = [((float(p) / float(observ[0])) - 1) for p in observ]\n",
        "    return normalized_window\n",
        "\n",
        "  def action_spec(self):\n",
        "    # action = 0 --> sell\n",
        "    # action = 1 --> buy\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def _reset(self):\n",
        "    self._episode_ended = False\n",
        "    self._current_index = self._start_index\n",
        "    self._state = self._get_observation()\n",
        "    return ts.restart(np.array(self._state, dtype=np.float32))\n",
        "\n",
        "  def _step(self, action):\n",
        "    if self._episode_ended:\n",
        "      return self.reset()\n",
        "\n",
        "    reward = self._calculate_reward(action)\n",
        "\n",
        "    if self._current_index == self._end_index:\n",
        "      self._episode_ended = True\n",
        "      self._current_index += 1\n",
        "      self._state = self._get_observation()\n",
        "      return ts.termination(np.array(self._state, dtype=np.float32), reward)\n",
        "\n",
        "    self._current_index += 1\n",
        "    self._state = self._get_observation()\n",
        "    return ts.transition(np.array(self._state, dtype=np.float32), reward , discount=1.0)\n",
        "\n",
        "  def _calculate_reward(self, action):\n",
        "    step_reward = 0\n",
        "    now=self._prices[self._current_index]\n",
        "    next=self._prices[self._current_index + 1]\n",
        "    diff=(now-next)\n",
        "    if action==0 :\n",
        "      step_reward+=diff\n",
        "    elif action==1 :\n",
        "      step_reward-=diff\n",
        "    else:\n",
        "      raise ValueError('`action` should be 0 or 1.')\n",
        "    \n",
        "    return step_reward\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkc4KUHeAw0s",
        "colab_type": "text"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrJKwA7eAf51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_iterations = 1000000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 10000  # @param {type:\"integer\"} \n",
        "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 1000000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 64  # @param {type:\"integer\"}\n",
        "learning_rate = 1e-3  # @param {type:\"number\"}\n",
        "log_interval = 200  # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
        "eval_interval = 1000  # @param {type:\"integer\"}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jit3o45JA9VM",
        "colab_type": "text"
      },
      "source": [
        "Dateset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B32rGl6PA_DN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "dataset = pd.read_csv('/content/drive/My Drive/Colab Notebooks/googl.us.csv', parse_dates=['Date'])\n",
        "dataset = dataset[['Date', 'Close']]\n",
        "dataset.set_index('Date', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dDDsrukIhi6",
        "colab_type": "text"
      },
      "source": [
        "Create python Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOkGarpbBNor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_py_env = StockMarket(dataset=dataset, start_date='2015-01-01', end_date='2016-11-30', window_size=30)\n",
        "eval_py_env = StockMarket(dataset=dataset, start_date='2017-01-01', end_date='2017-11-10', window_size=30)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_TKiVM0IphZ",
        "colab_type": "text"
      },
      "source": [
        "Convert python env to tensorflow env"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDpa5r4IBPfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlOtSqvvIuj4",
        "colab_type": "text"
      },
      "source": [
        "Create DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bO3q29HBRwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessing=tf.keras.layers.Lambda(\n",
        "    lambda obs: (tf.cast(obs , np.float32)-train_py_env.time_step_spec().observation.minimum)/(train_py_env.time_step_spec().observation.maximum- train_py_env.time_step_spec().observation.minimum)\n",
        ")\n",
        "q_net = q_rnn_network.QRnnNetwork(\n",
        "    train_env.observation_spec(),\n",
        "    train_env.action_spec(),\n",
        "    preprocessing_layers=preprocessing,\n",
        "    input_fc_layer_params=None,\n",
        "    lstm_size=(30,30,),\n",
        "    output_fc_layer_params=(512,128,),\n",
        "    activation_fn=tf.keras.activations.relu)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4M6-vZ8IxCg",
        "colab_type": "text"
      },
      "source": [
        "Create Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgR4ouEEBXSg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_step_counter = tf.Variable(0)\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbdSEuKoI0mN",
        "colab_type": "text"
      },
      "source": [
        "Define policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XS9GY2lCHIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),train_env.action_spec())"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV11qGkSiV9Z",
        "colab_type": "text"
      },
      "source": [
        "Reply buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTJppBSYDTlb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=replay_buffer_max_length)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJs8YR2eif-n",
        "colab_type": "text"
      },
      "source": [
        "reply_observer Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-PoIrc3HrEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replay_observer = [replay_buffer.add_batch]\n",
        "\n",
        "def collect_step(environment, policy, buffer):\n",
        "  time_step = environment.current_time_step()\n",
        "  action_step = policy.action(time_step)\n",
        "  next_time_step = environment.step(action_step.action)\n",
        "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "\n",
        "  # Add trajectory to the replay buffer\n",
        "  buffer.add_batch(traj)\n",
        "\n",
        "def collect_data(env, policy, buffer, steps):\n",
        "  for _ in range(steps):\n",
        "    collect_step(env, policy, buffer)\n",
        "\n",
        "collect_data(train_env, random_policy, replay_buffer, steps=100)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuE95YtbI-An",
        "colab_type": "text"
      },
      "source": [
        "Create collect data driver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq8cesyOHZGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tf_agents.metrics import tf_metrics\n",
        "train_metrics = [tf_metrics.AverageReturnMetric()]\n",
        "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
        "collect_driver = DynamicStepDriver(\n",
        "    train_env,\n",
        "    collect_policy,\n",
        "    observers=replay_observer + train_metrics,\n",
        "    num_steps=collect_steps_per_iteration)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDM0ftUXJF2X",
        "colab_type": "text"
      },
      "source": [
        "Create initialize driver (collect random data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hnlZ_elKrO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init_driver = DynamicStepDriver(\n",
        "    train_env,\n",
        "    random_policy,\n",
        "    observers=replay_observer,\n",
        "    num_steps=initial_collect_steps)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOIYu2qEJN6q",
        "colab_type": "text"
      },
      "source": [
        "Create evaluate driver (test agent on eval_env)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_ajrVYILdBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tf_agents.metrics import tf_metrics\n",
        "eval_metrics = [tf_metrics.AverageReturnMetric()]\n",
        "\n",
        "from tf_agents.drivers.dynamic_episode_driver import DynamicEpisodeDriver\n",
        "eval_driver = DynamicEpisodeDriver(\n",
        "    eval_env,\n",
        "    eval_policy,\n",
        "    observers=eval_metrics,\n",
        "    num_episodes=num_eval_episodes)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnpoGFOfJces",
        "colab_type": "text"
      },
      "source": [
        "Generate init data using init_driver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbfzoJSZM8VI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init_driver.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwwiRx5qJjPO",
        "colab_type": "text"
      },
      "source": [
        "Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs6yNgLUNLpN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3, \n",
        "    sample_batch_size=batch_size, \n",
        "    num_steps=2).prefetch(3)\n",
        "iterator = iter(dataset)\n",
        "print(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvmB0_yxJl1k",
        "colab_type": "text"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF3GJ5yxS9Dc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "returns_eval = []\n",
        "returns_train = []\n",
        "\n",
        "agent.train = common.function(agent.train)\n",
        "collect_driver.run = common.function(collect_driver.run)\n",
        "eval_driver.run = common.function(eval_driver.run)\n",
        "time_step = None\n",
        "policy_state = agent.collect_policy.get_initial_state(train_env.batch_size)\n",
        "\n",
        "agent.train_step_counter.assign(0)\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
        "  time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "\n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    eval_driver.run()\n",
        "    print('step = {0}: Average Return (train) = {1}'.format(step, train_metrics[0].result()))\n",
        "    print('step = {0}: Average Return (eval) = {1}'.format(step, eval_metrics[0].result()))\n",
        "    returns_eval.append(eval_metrics[0].result())\n",
        "    returns_train.append(train_metrics[0].result())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-ksWl6KJoQg",
        "colab_type": "text"
      },
      "source": [
        "Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT1GyHVFE315",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iterations = range(1, num_iterations + 1, eval_interval)\n",
        "plt.plot(iterations, returns_train)\n",
        "plt.plot(iterations, returns_eval)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "plt.legend(['train', 'eval'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}