{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3-dqn-trade-env.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAdw5HyMhVk3",
        "colab_type": "text"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAeXQFFT1eKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install 'gym==0.10.11'\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install PILLOW\n",
        "!pip install 'pyglet==1.3.2'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install --pre tf-agents[reverb]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t0X2FERe7rE",
        "colab_type": "text"
      },
      "source": [
        "Import dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "804UkQdmiJmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "import pandas as pd\n",
        "from collections import deque\n",
        "from google.colab import drive\n",
        "from keras.layers import LSTM, Dense, Activation\n",
        "tf.compat.v1.enable_v2_behavior()"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "907d1PkKfIIP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Environment Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-arJlkJhiJ_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StockMarket(py_environment.PyEnvironment):\n",
        "\n",
        "  def __init__(self, dataset, start_date, end_date, window_size):\n",
        "      self._prices = dataset[start_date:end_date]['Close'].values\n",
        "      self._window_size = window_size\n",
        "      self._start_index = window_size-1\n",
        "      self._end_index = len(self._prices) - 2\n",
        "      self._current_index = self._start_index\n",
        "\n",
        "\n",
        "      self._action_spec = array_spec.BoundedArraySpec(\n",
        "          shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
        "      self._observation_spec = array_spec.BoundedArraySpec(\n",
        "          shape=(self._window_size,), dtype=np.float32, minimum=-1, maximum=1, name='observation')\n",
        "      self._state = self._get_observation()\n",
        "      self._episode_ended = False\n",
        "\n",
        "  def _get_observation(self):\n",
        "    observ=self._prices[self._current_index-self._window_size+1:self._current_index+1]\n",
        "    normalized_window = [((float(p) / float(observ[0])) - 1) for p in observ]\n",
        "    return normalized_window\n",
        "\n",
        "  def action_spec(self):\n",
        "    # action = 0 --> sell\n",
        "    # action = 1 --> buy\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def _reset(self):\n",
        "    self._episode_ended = False\n",
        "    self._current_index = self._start_index\n",
        "    self._state = self._get_observation()\n",
        "    return ts.restart(np.array(self._state, dtype=np.float32))\n",
        "\n",
        "  def _step(self, action):\n",
        "    if self._episode_ended:\n",
        "      return self.reset()\n",
        "\n",
        "    reward = self._calculate_reward(action)\n",
        "\n",
        "    if self._current_index == self._end_index:\n",
        "      self._episode_ended = True\n",
        "      self._current_index += 1\n",
        "      self._state = self._get_observation()\n",
        "      return ts.termination(np.array(self._state, dtype=np.float32), reward)\n",
        "\n",
        "    self._current_index += 1\n",
        "    self._state = self._get_observation()\n",
        "    return ts.transition(np.array(self._state, dtype=np.float32), reward , discount=1.0)\n",
        "\n",
        "  def _calculate_reward(self, action):\n",
        "    step_reward = 0\n",
        "    now=self._prices[self._current_index]\n",
        "    next=self._prices[self._current_index + 1]\n",
        "    diff=(now-next)\n",
        "    if action==0 :\n",
        "      step_reward+=diff\n",
        "    elif action==1 :\n",
        "      step_reward-=diff\n",
        "    else:\n",
        "      raise ValueError('`action` should be 0 or 1.')\n",
        "    \n",
        "    return step_reward"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d9-aiXYfRPy",
        "colab_type": "text"
      },
      "source": [
        "Import dataset and make enviroment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxOjmUk9iKKk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7470244a-c22f-43a2-91bd-d336ad0a8648"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "dataset = pd.read_csv('/content/drive/My Drive/Colab Notebooks/googl.us.csv', parse_dates=['Date'])\n",
        "dataset = dataset[['Date', 'Close']]\n",
        "dataset.set_index('Date', inplace=True)\n",
        "env = StockMarket(dataset=dataset, start_date='2015-01-01', end_date='2016-11-30', window_size=30)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX6bmObffiOu",
        "colab_type": "text"
      },
      "source": [
        "Q-Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBDJoJnAiKRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape = [30] # == env.observation_space.shape\n",
        "n_outputs = 2 # == env.action_space.n\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(LSTM(units=30, return_sequences=True, input_shape=(None,1)))\n",
        "model.add(Dense(units=32,activation='linear'))\n",
        "model.add(LSTM(units=30, return_sequences=False))\n",
        "model.add(Dense(n_outputs))   "
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KPeDyBkfutQ",
        "colab_type": "text"
      },
      "source": [
        "Reply buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IogWfkiziK36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replay_memory = deque(maxlen=1000000)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9OixtGmfv_c",
        "colab_type": "text"
      },
      "source": [
        "epsilon_greedy_policy Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWuDEfZ3iKN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epsilon_greedy_policy(state, epsilon=0):\n",
        "    if np.random.rand() < epsilon:\n",
        "      return np.random.randint(2)\n",
        "    else:\n",
        "      state=state.reshape((1,30,1))\n",
        "      Q_values = model.predict(state)\n",
        "      return np.argmax(Q_values[0])"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmx6HzWwf5Sz",
        "colab_type": "text"
      },
      "source": [
        "sample_experiences Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69z_FvvfiKJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_experiences(batch_size):\n",
        "    indices = np.random.randint(len(replay_memory), size=batch_size)\n",
        "    batch = [replay_memory[index] for index in indices]\n",
        "    states, actions, rewards, next_states = [\n",
        "        np.array([experience[field_index] for experience in batch])\n",
        "        for field_index in range(4)]\n",
        "    return states, actions, rewards, next_states\n"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XY574bRf-ke",
        "colab_type": "text"
      },
      "source": [
        "play_one_step Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5h2Z0JviJyW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_one_step(env, state, epsilon):\n",
        "    action = epsilon_greedy_policy(state, epsilon)\n",
        "    st = env.step(action)\n",
        "    next_state=st.observation\n",
        "    reward=float(st.reward)\n",
        "    replay_memory.append((state, action, reward, next_state))\n",
        "    return next_state, reward "
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSQWIKAYgEYa",
        "colab_type": "text"
      },
      "source": [
        "training_step Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UwxAlkTiJbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "discount_rate = 0.95\n",
        "optimizer = keras.optimizers.Adam(lr=1e-3)\n",
        "loss_fn = keras.losses.mean_squared_error\n",
        "\n",
        "def training_step(batch_size):\n",
        "    experiences = sample_experiences(batch_size)\n",
        "    states, actions, rewards, next_states = experiences\n",
        "    next_states=next_states.reshape(32,30,1)\n",
        "    next_Q_values = model.predict(next_states)\n",
        "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
        "    target_Q_values = (rewards +\n",
        "                       (1 - 0) * discount_rate * max_next_Q_values)\n",
        "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
        "    mask = tf.one_hot(actions, n_outputs)\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_Q_values = model(states)\n",
        "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
        "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFMbcvKMgLdA",
        "colab_type": "text"
      },
      "source": [
        "Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAPYQu3qJFjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rewards = [] \n",
        "best_score = 0\n",
        "for episode in range(600):\n",
        "  obs = env.reset().observation\n",
        "  epsilon = max(1 - episode / 600, 0.01)  \n",
        "  step,ret=0 ,0\n",
        "  while True:\n",
        "    time_step=env.current_time_step()\n",
        "    obs, reward = play_one_step(env, obs, epsilon)\n",
        "    ret+= reward\n",
        "    if time_step.is_last():\n",
        "      rewards.append(ret)\n",
        "      break\n",
        "    step+=1\n",
        "    print(\"\\rEpisode: {}, Steps: {}, eps: {:.3f} , return: {}\".format(episode, step + 1, epsilon , ret), end=\"\")\n",
        "  if episode > 50:\n",
        "    training_step(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4qc8izhgP9Q",
        "colab_type": "text"
      },
      "source": [
        "visualize "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQYROFVaJMNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(reward)\n",
        "plt.xlabel(\"Episode\", fontsize=14)\n",
        "plt.ylabel(\"Sum of rewards\", fontsize=14)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD-1iWzgga_Z",
        "colab_type": "text"
      },
      "source": [
        "Double DQN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mC_TdikxFOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "discount_rate = 0.95\n",
        "optimizer = keras.optimizers.Adam(lr=1e-3)\n",
        "loss_fn = keras.losses.mean_squared_error\n",
        "\n",
        "target = keras.models.clone_model(model)\n",
        "target.set_weights(model.get_weights())\n",
        "\n",
        "def training_step(batch_size):\n",
        "  experiences = sample_experiences(batch_size)\n",
        "  states, actions, rewards, next_states = experiences\n",
        "  next_states=next_states.reshape(32,30,1)\n",
        "  next_Q_values = model.predict(next_states)\n",
        "  best_next_actions = np.argmax(next_Q_values, axis=1)\n",
        "  next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n",
        "  next_best_Q_values = (target.predict(next_states) * next_mask).sum(axis=1)\n",
        "  target_Q_values = (rewards +\n",
        "  (1 - 0) * discount_rate * next_best_Q_values)\n",
        "  mask = tf.one_hot(actions, n_outputs)\n",
        "  with tf.GradientTape() as tape:\n",
        "      all_Q_values = model(states)\n",
        "      Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
        "      loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JJMTOnFxGef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rewards = [] \n",
        "best_score = 0\n",
        "\n",
        "for episode in range(600):\n",
        "  obs = env.reset().observation\n",
        "  epsilon = max(1 - episode / 600, 0.01)  \n",
        "  step,ret=0 ,0\n",
        "  while True:\n",
        "    time_step=env.current_time_step()\n",
        "    obs, reward = play_one_step(env, obs, epsilon)\n",
        "    ret+= reward\n",
        "    if time_step.is_last():\n",
        "      rewards.append(ret)\n",
        "      break\n",
        "    step+=1\n",
        "    print(\"\\rEpisode: {}, Steps: {}, eps: {:.3f} , return: {}\".format(episode, step + 1, epsilon , ret), end=\"\")\n",
        "  if episode > 50:\n",
        "    training_step(batch_size)\n",
        "  if episode % 50 == 0:\n",
        "    target.set_weights(model.get_weights())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc56du_Ngeub",
        "colab_type": "text"
      },
      "source": [
        "Dueling DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mfF-So4gjPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K = keras.backend\n",
        "input_states = keras.layers.Input(shape=[4])\n",
        "hidden1 = keras.layers.Dense(32, activation=\"elu\")(input_states)\n",
        "hidden2 = keras.layers.Dense(32, activation=\"elu\")(hidden1)\n",
        "state_values = keras.layers.Dense(1)(hidden2)\n",
        "raw_advantages = keras.layers.Dense(n_outputs)(hidden2)\n",
        "advantages = raw_advantages - K.max(raw_advantages, axis=1, keepdims=True)\n",
        "Q_values = state_values + advantages\n",
        "model = keras.models.Model(inputs=[input_states], outputs=[Q_values])\n",
        "\n",
        "target = keras.models.clone_model(model)\n",
        "target.set_weights(model.get_weights())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8C69tcMgp4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "discount_rate = 0.95\n",
        "optimizer = keras.optimizers.Adam(lr=1e-2)\n",
        "loss_fn = keras.losses.Huber()\n",
        "\n",
        "def training_step(batch_size):\n",
        "    experiences = sample_experiences(batch_size)\n",
        "    states, actions, rewards, next_states = experiences\n",
        "    next_states=next_states.reshape(32,30,1)\n",
        "    next_Q_values = model.predict(next_states)\n",
        "    best_next_actions = np.argmax(next_Q_values, axis=1)\n",
        "    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n",
        "    next_best_Q_values = (target.predict(next_states) * next_mask).sum(axis=1)\n",
        "    target_Q_values = (rewards + \n",
        "                       (1 - 0) * discount_rate * next_best_Q_values)\n",
        "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
        "    mask = tf.one_hot(actions, n_outputs)\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_Q_values = model(states)\n",
        "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
        "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI1PjNC9gtY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rewards = [] \n",
        "best_score = 0\n",
        "\n",
        "for episode in range(600):\n",
        "  obs = env.reset().observation\n",
        "  epsilon = max(1 - episode / 600, 0.01)  \n",
        "  step,ret=0 ,0\n",
        "  while True:\n",
        "    time_step=env.current_time_step()\n",
        "    obs, reward = play_one_step(env, obs, epsilon)\n",
        "    ret+= reward\n",
        "    if time_step.is_last():\n",
        "      rewards.append(ret)\n",
        "      break\n",
        "    step+=1\n",
        "    print(\"\\rEpisode: {}, Steps: {}, eps: {:.3f} , return: {}\".format(episode, step + 1, epsilon , ret), end=\"\")\n",
        "  if episode > 50:\n",
        "    training_step(batch_size)\n",
        "  if episode % 200 == 0:\n",
        "    target.set_weights(model.get_weights())\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}